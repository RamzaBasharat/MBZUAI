
- title: "HyperLoader: Integrating Hypernetwork-Based LoRA and Adapter Layers into Multi-Task Transformers for Sequence Labelling"
  authors: "Jesus-German Ortiz-Barajas, Helena Gomez-Adorno, Thamar Solorio"
  abs: "We present HyperLoader, a simple approach that combines different parameter-efficient fine-tuning methods in a multi-task setting. To achieve this goal, our model uses a hypernetwork to generate the weights of these modules based on the task, the transformer layer, and its position within this layer. Our method combines the benefits of multi-task learning by capturing the structure of all tasks while reducing the task interference problem by encapsulating the task-specific knowledge in the generated weights and the benefits of combining different parameter-efficient methods to outperform full-fine tuning. We provide empirical evidence that HyperLoader outperforms previous approaches in most datasets and obtains the best average performance across tasks in high-resource and low-resource scenarios."
  publisher: "arXiv preprint arXiv:2407.01411"
  date: "2024/7/1"
  url: "https://arxiv.org/abs/2407.01411"
  pdf: "https://arxiv.org/pdf/2407.01411"
  bibtex: |
    @article{ortiz2024hyperloader,
      title={HyperLoader: Integrating Hypernetwork-Based LoRA and Adapter Layers into Multi-Task Transformers for Sequence Labelling},
      author={Ortiz-Barajas, Jesus-German and Gomez-Adorno, Helena and Solorio, Thamar},
      journal={arXiv preprint arXiv:2407.01411},
      year={2024}
    }

- title: "Multimodal-Attention Fusion for the Detection of Questionable Content in Videos"
  authors: "Arnold Morales, Elaheh Baharlouei, Thamar Solorio, Hugo Jair Escalante"
  abs: "We address the problem of questionable content filtering from videos, in particular, we focus on the detection of comic mischief. Attention-based models have been proposed to approach this problem, mostly relying on hierarchical cross-attention (HCA) for fusing multimodal information. While competitive performance has been obtained with such solutions, it is unclear whether the hierarchical mechanism is the best choice for this type of model. We explore in this paper the use of an alternative mechanism called parallel cross-attention (ParCA). Also, we propose the use of gated multimodal units (GMU) for fusing multiple multimodal attention mechanisms, besides the traditional concatenation. Experimental results show that the combination of parallel cross-attention and the use GMU improves considerably the performance of the reference model based on HCA."
  publisher: "Mexican Conference on Pattern Recognition, 2024 Springer"
  url: "https://link.springer.com/chapter/10.1007/978-3-031-62836-8_18"
  date: "2024/6/17"
  bibtex: |
    @inproceedings{morales2024multimodal,
      title={Multimodal-Attention Fusion for the Detection of Questionable Content in Videos},
      author={Morales, Arnold and Baharlouei, Elaheh and Solorio, Thamar and Escalante, Hugo Jair},
      booktitle={Mexican Conference on Pattern Recognition},
      pages={188--199},
      year={2024},
      organization={Springer}
    }

- title: ""
  authors: ""
  abs: ""
  publisher: ""
  url: ""
  pdf: ""
  date: ""
  bibtex: |

- title: ""
  authors: ""
  abs: ""
  publisher: ""
  url: ""
  pdf: ""
  date: ""
  bibtex: |

- title: ""
  authors: ""
  abs: ""
  publisher: ""
  url: ""
  pdf: ""
  date: ""
  bibtex: |

- title: ""
  authors: ""
  abs: ""
  publisher: ""
  url: ""
  pdf: ""
  date: ""
  bibtex: |

- title: ""
  authors: ""
  abs: ""
  publisher: ""
  url: ""
  pdf: ""
  date: ""
  bibtex: |

- title: ""
  authors: ""
  abs: ""
  publisher: ""
  url: ""
  pdf: ""
  date: ""
  bibtex: |

- title: ""
  authors: ""
  abs: ""
  publisher: ""
  url: ""
  pdf: ""
  date: ""
  bibtex: |

- title: ""
  authors: ""
  abs: ""
  publisher: ""
  url: ""
  pdf: ""
  date: ""
  bibtex: |

- title: ""
  authors: ""
  abs: ""
  publisher: ""
  url: ""
  pdf: ""
  date: ""
  bibtex: |

- title: ""
  authors: ""
  abs: ""
  publisher: ""
  url: ""
  pdf: ""
  date: ""
  bibtex: |

- title: ""
  authors: ""
  abs: ""
  publisher: ""
  url: ""
  pdf: ""
  date: ""
  bibtex: |

- title: ""
  authors: ""
  abs: ""
  publisher: ""
  url: ""
  pdf: ""
  date: ""
  bibtex: |

- title: ""
  authors: ""
  abs: ""
  publisher: ""
  url: ""
  pdf: ""
  date: ""
  bibtex: |

- title: ""
  authors: ""
  abs: ""
  publisher: ""
  url: ""
  pdf: ""
  date: ""
  bibtex: |

- title: ""
  authors: ""
  abs: ""
  publisher: ""
  url: ""
  pdf: ""
  date: ""
  bibtex: |

- title: ""
  authors: ""
  abs: ""
  publisher: ""
  url: ""
  pdf: ""
  date: ""
  bibtex: |

- title: ""
  authors: ""
  abs: ""
  publisher: ""
  url: ""
  pdf: ""
  date: ""
  bibtex: |
