
- title: "HyperLoader: Integrating Hypernetwork-Based LoRA and Adapter Layers into Multi-Task Transformers for Sequence Labelling"
  authors: "Jesus-German Ortiz-Barajas, Helena Gomez-Adorno, Thamar Solorio"
  abs: "We present HyperLoader, a simple approach that combines different parameter-efficient fine-tuning methods in a multi-task setting. To achieve this goal, our model uses a hypernetwork to generate the weights of these modules based on the task, the transformer layer, and its position within this layer. Our method combines the benefits of multi-task learning by capturing the structure of all tasks while reducing the task interference problem by encapsulating the task-specific knowledge in the generated weights and the benefits of combining different parameter-efficient methods to outperform full-fine tuning. We provide empirical evidence that HyperLoader outperforms previous approaches in most datasets and obtains the best average performance across tasks in high-resource and low-resource scenarios."
  publisher: "arXiv preprint arXiv:2407.01411"
  date: "2024/7/1"
  url: "https://arxiv.org/abs/2407.01411"
  pdf: "https://arxiv.org/pdf/2407.01411"
  bibtex: |
    @article{ortiz2024hyperloader,
      title={HyperLoader: Integrating Hypernetwork-Based LoRA and Adapter Layers into Multi-Task Transformers for Sequence Labelling},
      author={Ortiz-Barajas, Jesus-German and Gomez-Adorno, Helena and Solorio, Thamar},
      journal={arXiv preprint arXiv:2407.01411},
      year={2024}
    }

- title: "Multimodal-Attention Fusion for the Detection of Questionable Content in Videos"
  authors: "Arnold Morales, Elaheh Baharlouei, Thamar Solorio, Hugo Jair Escalante"
  abs: "We address the problem of questionable content filtering from videos, in particular, we focus on the detection of comic mischief. Attention-based models have been proposed to approach this problem, mostly relying on hierarchical cross-attention (HCA) for fusing multimodal information. While competitive performance has been obtained with such solutions, it is unclear whether the hierarchical mechanism is the best choice for this type of model. We explore in this paper the use of an alternative mechanism called parallel cross-attention (ParCA). Also, we propose the use of gated multimodal units (GMU) for fusing multiple multimodal attention mechanisms, besides the traditional concatenation. Experimental results show that the combination of parallel cross-attention and the use GMU improves considerably the performance of the reference model based on HCA."
  publisher: "Mexican Conference on Pattern Recognition, 2024 Springer"
  url: "https://link.springer.com/chapter/10.1007/978-3-031-62836-8_18"
  date: "2024/6/17"
  bibtex: |
    @inproceedings{morales2024multimodal,
      title={Multimodal-Attention Fusion for the Detection of Questionable Content in Videos},
      author={Morales, Arnold and Baharlouei, Elaheh and Solorio, Thamar and Escalante, Hugo Jair},
      booktitle={Mexican Conference on Pattern Recognition},
      pages={188--199},
      year={2024},
      organization={Springer}
    }

- title: "Labeling Comic Mischief Content in Online Videos with a Multimodal Hierarchical-Cross-Attention Model"
  authors: "Elaheh Baharlouei, Mahsa Shafaei, Yigeng Zhang, Hugo Jair Escalante, Thamar Solorio"
  abs: "We address the challenge of detecting questionable content in online media, specifically the subcategory of comic mischief. This type of content combines elements such as violence, adult content, or sarcasm with humor, making it difficult to detect. Employing a multimodal approach is vital to capture the subtle details inherent in comic mischief content. To tackle this problem, we propose a novel end-to-end multimodal system for the task of comic mischief detection. As part of this contribution, we release a novel dataset for the targeted task consisting of three modalities: video, text (video captions and subtitles), and audio. We also design a HIerarchical Cross-attention model with CAPtions (HICCAP) to capture the intricate relationships among these modalities. The results show that the proposed approach makes a significant improvement over robust baselines and state-of-the-art models for comic mischief detection and its type classification. This emphasizes the potential of our system to empower users, to make informed decisions about the online content they choose to see. In addition, we conduct experiments on the UCF101, HMDB51, and XD-Violence datasets, comparing our model against other state-of-the-art approaches showcasing the outstanding performance of our proposed model in various scenarios."
  publisher: "arXiv preprint arXiv:2406.07841"
  url: "https://arxiv.org/abs/2406.07841"
  pdf: "https://arxiv.org/pdf/2406.07841"
  date: "2024/6/12"
  bibtex: |
    @article{baharlouei2024labeling,
      title={Labeling Comic Mischief Content in Online Videos with a Multimodal Hierarchical-Cross-Attention Model},
      author={Baharlouei, Elaheh and Shafaei, Mahsa and Zhang, Yigeng and Escalante, Hugo Jair and Solorio, Thamar},
      journal={arXiv preprint arXiv:2406.07841},
      year={2024}
    }

- title: "CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark"
  authors: "David Romero, Chenyang Lyu, Haryo Akbarianto Wibowo, Teresa Lynn, Injy Hamed, Aditya Nanda Kishore, Aishik Mandal, Alina Dragonetti, Artem Abzaliev, Atnafu Lambebo Tonja, Bontu Fufa Balcha, Chenxi Whitehouse, Christian Salamea, Dan John Velasco, David Ifeoluwa Adelani, David Le Meur, Emilio Villa-Cueva, Fajri Koto, Fauzan Farooqui, Frederico Belcavello, Ganzorig Batnasan, Gisela Vallejo, Grainne Caulfield, Guido Ivetta, Haiyue Song, Henok Biadglign Ademtew, Hernán Maina, Holy Lovenia, Israel Abebe Azime, Jan Christian Blaise Cruz, Jay Gala, Jiahui Geng, Jesus-German Ortiz-Barajas, Jinheon Baek, Jocelyn Dunstan, Laura Alonso Alemany, Kumaranage Ravindu Yasas Nagasinghe, Luciana Benotti, Luis Fernando D'Haro, Marcelo Viridiano, Marcos Estecha-Garitagoitia, Maria Camila Buitrago Cabrera, Mario Rodríguez-Cantelar, Mélanie Jouitteau, Mihail Mihaylov, Mohamed Fazli Mohamed Imam, Muhammad Farid Adilazuarda, Munkhjargal Gochoo, Munkh-Erdene Otgonbold, Naome Etori, Olivier Niyomugisha, Paula Mónica Silva, Pranjal Chitale, Raj Dabre, Rendi Chevi, Ruochen Zhang, Ryandito Diandaru, Samuel Cahyawijaya, Santiago Góngora, Soyeong Jeong, Sukannya Purkayastha, Tatsuki Kuribayashi, Thanmay Jayakumar, Tiago Timponi Torrent, Toqeer Ehsan, Vladimir Araujo, Yova Kementchedjhieva, Zara Burzo, Zheng Wei Lim, Zheng Xin Yong, Oana Ignat, Joan Nwatu, Rada Mihalcea, Thamar Solorio, Alham Fikri Aji"
  abs: "Visual Question Answering (VQA) is an important task in multimodal AI, and it is often used to test the ability of vision-language models to understand and reason on knowledge present in both visual and textual data. However, most of the current VQA models use datasets that are primarily focused on English and a few major world languages, with images that are typically Western-centric. While recent efforts have tried to increase the number of languages covered on VQA datasets, they still lack diversity in low-resource languages. More importantly, although these datasets often extend their linguistic range via translation or some other approaches, they usually keep images the same, resulting in narrow cultural representation. To address these limitations, we construct CVQA, a new Culturally-diverse multilingual Visual Question Answering benchmark, designed to cover a rich set of languages and cultures, where we engage native speakers and cultural experts in the data collection process. As a result, CVQA includes culturally-driven images and questions from across 28 countries on four continents, covering 26 languages with 11 scripts, providing a total of 9k questions. We then benchmark several Multimodal Large Language Models (MLLMs) on CVQA, and show that the dataset is challenging for the current state-of-the-art models. This benchmark can serve as a probing evaluation suite for assessing the cultural capability and bias of multimodal models and hopefully encourage more research efforts toward increasing cultural awareness and linguistic diversity in this field."
  publisher: "arXiv preprint arXiv:2406.05967"
  url: "https://arxiv.org/abs/2406.05967"
  pdf: "https://arxiv.org/pdf/2406.05967"
  date: "2024/6/10"
  bibtex: |
    @article{romero2024cvqa,
      title={CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark},
      author={Romero, David and Lyu, Chenyang and Wibowo, Haryo Akbarianto and Lynn, Teresa and Hamed, Injy and Kishore, Aditya Nanda and Mandal, Aishik and Dragonetti, Alina and Abzaliev, Artem and Tonja, Atnafu Lambebo and others},
      journal={arXiv preprint arXiv:2406.05967},
      year={2024}
    }

- title: "The Privileged Students: On the Value of Initialization in Multilingual Knowledge Distillation"
  authors: "Haryo Akbarianto Wibowo, Thamar Solorio, Alham Fikri Aji"
  date: "2024/6"
  abs: "Knowledge distillation (KD) has proven to be a successful strategy to improve the performance of a smaller model in many NLP tasks. However, most of the work in KD only explores monolingual scenarios. In this paper, we investigate the value of KD in multilingual settings. We find the significance of KD and model initialization by analyzing how well the student model acquires multilingual knowledge from the teacher model. Our proposed method emphasizes copying the teacher model's weights directly to the student model to enhance initialization. Our finding shows that model initialization using copy-weight from the fine-tuned teacher contributes the most compared to the distillation process itself across various multilingual settings. Furthermore, we demonstrate that efficient weight initialization preserves multilingual capabilities even in low-resource scenarios."
  publisher: "eprint arXiv:2406.16524"
  url: "https://ui.adsabs.harvard.edu/abs/2024arXiv240616524A/abstract"
  bibtex: |
    @article{wibowo2024privileged,
      title={The Privileged Students: On the Value of Initialization in Multilingual Knowledge Distillation},
      author={Wibowo, Haryo Akbarianto and Solorio, Thamar and Aji, Alham Fikri},
      journal={arXiv preprint arXiv:2406.16524},
      year={2024}
    }

- title: "Adaptive Cross-lingual Text Classification through In-Context One-Shot Demonstrations"
  authors: "Emilio Cueva, Adrian Lopez Monroy, Fernando Sánchez-Vega, Thamar Solorio"
  date: "2024/6"
  abs: "Zero-Shot Cross-lingual Transfer (ZS-XLT) utilizes a model trained in a source language to make predictions in another language, often with a performance loss. To alleviate this, additional improvements can be achieved through subsequent adaptation using examples in the target language. In this paper, we exploit In-Context Tuning (ICT) for One-Shot Cross-lingual transfer in the classification task by introducing In-Context Cross-lingual Transfer (IC-XLT). The novel concept involves training a model to learn from context examples and subsequently adapting it during inference to a target language by prepending a One-Shot context demonstration in that language. Our results show that IC-XLT successfully leverages target-language examples to improve the cross-lingual capabilities of the evaluated mT5 model, outperforming prompt-based models in the Zero and Few-shot scenarios adapted through fine-tuning. Moreover, we show that when source-language data is limited, the fine-tuning framework employed for IC-XLT performs comparably to prompt-based fine-tuning with significantly more training data in the source language."
  publisher: "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)"
  url: "https://aclanthology.org/2024.naacl-long.460/"
  bibtex: |
    @article{villa2024adaptive,
      title={Adaptive Cross-lingual Text Classification through In-Context One-Shot Demonstrations},
      author={Villa-Cueva, Emilio and L{\'o}pez-Monroy, A Pastor and S{\'a}nchez-Vega, Fernando and Solorio, Thamar},
      journal={arXiv preprint arXiv:2404.02452},
      year={2024}
    }
